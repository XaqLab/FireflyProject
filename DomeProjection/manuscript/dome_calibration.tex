\documentclass{article}
%\documentclass[journal=jpccck,manuscript=article]{achemso}

\usepackage{fixltx2e}		% used for superscripts and subscripts
\usepackage{textcomp}		% used for degree symbol
\usepackage{graphicx}  		% used to import graphics
\graphicspath{{./figures/}}
\usepackage{caption}  		% used to format figure captions
\usepackage[percent]{overpic}	% used to annotate pictures
\usepackage{authblk}

% title, authors, and affiliations
\title{Viewing Direction Calibration \\
                 in a \\
	 Murine Virtual Reality System}
\author[1,2]{James W. Bridgewater\thanks{james.bridgewater@bcm.edu}}
\author[1]{Reuben H. Fan}
\author[1]{Dora E. Angelaki}
\author[1,2]{Xaq S. Pitkow}
\affil[1]{Department of Neuroscience, Baylor College of Medicine}
\affil[2]{Department of Electrical and Computer Engineering, Rice University}

\renewcommand\Authands{ and }


\begin{document}  % the document starts here

\maketitle

\begin{abstract}

For a virtual reality system to be capable of engaging a subject in
navigational tasks, the visual stimuli it presents must cover the vast majority
of the subject's visual field. This is especially challenging with rodents due
the their exceptionally large visual fields.  Several research groups have
reported using virtual reality systems with rodents, but none have published a
method for minimizing or even measuring the viewing direction errors in their
visual stimuli. Here we detail a calibration technique that both minimizes and
quantifies the viewing direction errors in the visual stimuli of our murine
virtual reality system.

\end{abstract}


\section{Introduction}

Virtual reality systems provide neuroscience researchers the ability to
investigate animals' navigational capabilities in a virtual environment that
can be modified far easier than the physical environment in the laboratory.
They also enable the investigation of navigation in head-fixed animals thereby
facilitating the use of neural recording and neural imaging systems that can be
difficult to use in freely moving animals.  The combination of virtual reality
and neural monitoring have made it possible to investigate the neural
representations of the world in awake, behaving animals engaged in complex
navigational tasks.

\includegraphics[width=1.0\linewidth]{dome_setup}

Virtual reality systems have long been used in neuroscientific experimentation
to engage primates in navigational behavior, however, attempts to get rodents
to navigate virtual reality environments were unsuccessful until a virtual
reality system was designed which accounted for the much larger field of view
of the murine visual system \cite{holscher2005rats, keller2012sensorimotor,
schmidt2013cellular, ayaz2013locomotion, saleem2013integration,
aronov2014engagement, rickgauer2014simultaneous, aghajan2015impaired}.  A
rodentâ€™s visual field covers nearly the entire half sphere above the horizon
and a large portion of the half sphere below it \cite{hughes1977topography,
wagor1980retinotopic, schuett2002mapping, wang2007area}.  To construct virtual
reality systems that cover such a large field of view, researchers typically
resort to either the use of multiple displays \cite{keller2012sensorimotor,
ayaz2013locomotion, saleem2013integration} or to the use of non-planar mirrors
to increase the visual field covered by a projector
\cite{harvey2009intracellular, schmidt2013cellular, aronov2014engagement}.  The
difficulty in using multiple displays lies in hiding their bezels. The use of
multiple displays has in some cases resulted in behavior which suggests that
the animal is navigating the real space occupied by the displays rather that
the virtual space shown on them \cite{holscher2005rats}.  The challenge of
using non-planar mirrors is distortion of the projected visual stimulus. This
distortion must be corrected by modifying the projected image to account for
the geometry of the projection system.  In principle, image modification is the
straightforward process of solving a 3-dimensional trigonometry problem to
achieve identical viewing directions. In practice the geometry of the system
must be known with sub-millimeter accuracy to achieve good results.  This is
difficult to accomplish via measurement because some elements of the system,
like the projector's focal point, are not directly observable.  Multiple
research groups have chosen to use non-planar mirrors in their murine virtual
reality systems to cover the large visual field required
\cite{holscher2005rats, harvey2009intracellular}, but none have published a
method for calibrating these systems or published metrics that
quantify the viewing direction errors in their system. In order to achieve a
highly accurate reproduction of viewing directions in our murine virtual
reality system, we have developed a calibration technique that utilizes a 3D
printed calibration device to project spots of light at several known viewing
directions. This device is placed at the animal's location inside the virtual
reality system and a program developed in house is then used to move green dots
around in the projector image until a green dot lies near the center of each
spot projected by the calibration device. This mapping of points in the
projector image to known viewing directions is then used to find values of the
geometric parameters of the virtual reality system that produce accurate
calculated viewing angles by searching for parameter values that minimize the
difference between the known viewing directions and the viewing directions
calculated for the points in the projector image that correspond to these known
viewing directions. Once these parameters are known, images can be modified
such that the projection of the modified image produces the original image.
This calibration method can also be used to quantify the error in viewing
directions as a function of location on our hemispherical display.  


\section{The Calculation of Viewing Directions}

Producing the desired viewing directions in the virtual reality system requires 
the ability to calculate the direction in which the animal sees any point in
the projector image when the image is projected, via the spherical mirror, onto
the hemispherical screen.  This is done in two steps as illustrated in
Figure\ref{calc_viewing_direction}. First the vector along which light is
projected for the given point is determined from the properties of the
projector, the point is found where this vector strikes the spherical mirror
and a vector corresponding to the reflected light is found. Secondly the point
is found where the reflected light hits the hemispherical screen and used to
calculate the vector from the animal's head to this point on the screen. This
vector is normalized to produce a unit vector which describes the viewing
direction corresponding to the point in the projector's image.

\includegraphics[width=1.0\linewidth]{forward_mapping}

This calculation requires quantitative values for several geometric propeties
of this virtual reality system. Specifically the location of the projector's
focal point, the aspect ratio of its image, an angle quantifying how much the
image spreads out horizontally as it gets farther from the projector and a
slope describing how far the image rises as it gets farther from the projector.
Also required are the radius of the spherical mirror, the radius of the
hemispherical screen, the location of the screen's center and the location of
the animal's head. It proved to be exceedingly difficult to obtain sufficiently
accurate values for all of these parameters through direct measurement.
Consider for instance that the focal point of the projector, the center of the
hemispherical screen and the center of the spherical mirror (the origin of our
chosen coordinate system) are not directly observable. We therefore developed
the calibration method described in the next section to find values of these
parameters which result in accurate calculated viewing directions.


\section{The Calibration Process}

After initially pursuing a camera based calibration method, we moved to a much
simpler method that utilizes a 3D printed calibration device. Camera based
methods require either fisheye lenses or numerous overlapping photographs (or
both) to achieve the wide field of view required to calibrate a murine virtual
reality system and correcting for camera distortion and the movement of the
camera's focal point between photos becomes complicated.  The calibration
device we use projects 37 spots of light in known directions, four rows of nine
spots plus one spot straight overhead. Each row of 9 spots has spots at yaw
angles of -120, -90, -60, -30, 0, 30, 60, 90, and 120 degrees where 0 degrees
is straight ahead, negative angles are to the left and positive angles are to
the right.  The four rows are at pitch angles of -15, 0, 30, and 60 degrees
where negative angles are downward and positive angles are upward. The
additional spot straight overhead has a pitch angle of 90 degrees. This
calibration device is placed at the animal's location inside the virtual
reality system and software developed specifically for this calibration method
is used to move green dots around in the projector's image until there's a
green dot in the center of each spot projected by the calibration device. This
software is then used to save a file containing the location of all the green
dots. This file constitutes a mapping between 37 points in the projector image
and 37 known viewing directions which can be used to find virtual reality
system geometry parameter values which produce accurate viewing directions
using the calculation procedure described in section\ref{}.  These parameter
values are found using an optimization method which minimizes the sum of the
square L2 norms of the vectors which constitute the differences between the
calculated and known viewing directions.

Insert table of viewing direction differences


\section{Image Modification}

\section{Conclusions}

\section{Acknowledgements}

This work was supported by NSF IOS-1450923, the Simons Foundation, and the McNair Foundation.

\bibliographystyle{plain}
\bibliography{dome_calibration}

\captionsetup[figure]{labelformat=empty} % get rid of "Figure" for empty captions

\pagebreak

\textbf{Figures}
\begin{enumerate}
  \item geometry of our murine virtual reality system
  \item virtual reality geometry with triangles to demonstrate forward mapping
  \item image of calibration device
  \item photo of dome during calibration with white spots, green dots
  \item photo of dome after calibration with white spots, green dots
  \item table of viewing direction errors for each calibration direction
  \item image, picture of image projected
  \item warped image, picture of warped image projected
  \item image, picture of warped image projected
\end{enumerate}

%\input{figures}

\end{document}
