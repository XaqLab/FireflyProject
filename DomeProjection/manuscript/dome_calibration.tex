% Template for PLoS
% Version 3.3 June 2016
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this
% will help to maintain a clean tex file.  Visit
% https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at
% latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not
% use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they
% are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF
%   before submission. 
% - Figures containing multiple panels/subfigures must be combined into one
%   image file before submission.
% For figure citations, please use "Fig" instead of "Figure".  See
% http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular
%   environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth
% environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT Below are a few tips to help format your equations and other
% special characters according to our specifications. For more tips to help
% reduce the possibility of formatting errors during conversion, please see our
% LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation
% in the math environment.  For example, x$^2$ is incorrect; this should be
% formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example,
% CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to
% fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within
% the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please
% group using {}.  For example, change "[U(D,E,\gamma)]^2" to
% "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{27.023pt}
\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\sf PLOS}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Calibration of a Murine Virtual Reality System}
}
\newline
% Insert author names, affiliations and corresponding author email (do not
% include titles, positions, or degrees).
\\
James W. Bridgewater\textsuperscript{1,2},
Reuben H. Fan\textsuperscript{1},
Dora E. Angelaki\textsuperscript{1},
Xaq S. Pitkow\textsuperscript{1,2*}
\\
\bigskip
\textbf{1} Department of Neuroscience, Baylor College of Medicine, Houston, TX,
USA
\\
\textbf{2} Department of Electrical and Computer Engineering, Rice University,
Houston, TX, USA
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert
% symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
%\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as
% senior authorship.
%\ddag These authors also contributed equally to this work.

% Current address notes
%\textcurrency Current Address: Dept/Program/Center, Institution Name, City,
%State, Country % change symbol to "\textcurrency a" if more than one current
%address note \textcurrency b Insert second current address \textcurrency c
%Insert third current address

% Deceased author note
%\dag Deceased

% Group/Consortium Author Note
%\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address
% in note below.
* xaq@rice.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
For a virtual reality system to be capable of engaging a subject in
navigational tasks, the visual stimuli it presents must cover the vast majority
of the subject's visual field. This is challenging with rodents due
to their exceptionally large visual fields.  Several research groups have reported using
virtual reality systems with rodents, but none have published a method for
minimizing or even measuring the viewing direction errors in their visual
stimuli. Here we detail a calibration technique that both minimizes and
quantifies the viewing direction errors in the visual stimuli of a murine
virtual reality system.


% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
%\section*{Author Summary}

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
Virtual reality systems provide neuroscience researchers the ability to
investigate animals' navigational capabilities in a virtual environment that
can be modified far easier than the physical environment in the laboratory.
They also enable the investigation of navigation in head-fixed animals thereby
facilitating the use of neural recording and neural imaging systems that can be
difficult to use in freely moving animals.  The combination of virtual reality
and neural monitoring have made it possible to investigate the neural
representations of the world in awake, behaving animals performing complex
navigational tasks.

Virtual reality systems have long been used in neuroscientific experimentation
to engage primates in navigational behavior, however, attempts to get rodents
to navigate virtual reality environments were unsuccessful until a virtual
reality system was designed which accounted for the much larger field of view
of the murine visual system \cite{holscher2005rats, keller2012sensorimotor,
schmidt2013cellular, ayaz2013locomotion, saleem2013integration,
aronov2014engagement, rickgauer2014simultaneous, aghajan2015impaired}.  A
rodentâ€™s visual field covers nearly the entire half sphere above the horizon
and a large portion of the half sphere below it \cite{hughes1977topography,
wagor1980retinotopic, schuett2002mapping, wang2007area}.  To construct virtual
reality systems that cover such a large field of view, researchers typically
resort to either the use of multiple displays \cite{keller2012sensorimotor,
ayaz2013locomotion, saleem2013integration} or to the use of non-planar mirrors
to increase the visual field covered by a projector
\cite{harvey2009intracellular, schmidt2013cellular, aronov2014engagement}.  The
difficulty in using multiple displays lies in hiding their bezels. The use of
multiple displays has in some cases resulted in behavior which suggests that
the animal is navigating the real space occupied by the displays rather that
the virtual space shown on them \cite{holscher2005rats}.  The challenge of
using non-planar mirrors is distortion of the projected visual stimuli. This
distortion must be corrected by modifying the projected image to account for
the geometry of the projection system.  In principle image modification is the
straightforward process of solving a 3-dimensional trigonometry problem to
achieve the desired viewing directions. In practice the geometry of the system
must be known with sub-millimeter accuracy to achieve good results.  This is
difficult to accomplish via measurement because some elements of the system,
like the projector's focal point, are not directly observable.  

Multiple research groups have chosen to use non-planar mirrors in their murine
virtual reality systems to cover the large visual field required
\cite{holscher2005rats, harvey2009intracellular}, but none have published a
method for calibrating these systems or published metrics that quantify the
viewing direction errors in their system. In order to achieve a highly accurate
reproduction of viewing directions in our murine virtual reality system, we
have developed a calibration technique that utilizes a 3D printed calibration
device to project spots of light in 37 known viewing directions. This device is
placed at the animal's location inside the virtual reality system and a program
developed in house is then used to move green dots around in the projector
image until a green dot lies near the center of each spot projected by the
calibration device. This mapping of points in the projector image to known
viewing directions is then used to find values of the virtual reality system's
geometric parameters that produce accurate calculated viewing directions by
minimizing the difference between the known and calculated viewing directions.
Once these parameters are known, an image can be modified such that 
projection of the modified image in the virtual reality system produces the
original image.

\section*{Materials and Methods}

\subsection*{System Geometry}

Fig\ref{dome_setup} is a depiction of our murine virtual reality
system. The visual stimuli from the projector reflect off a spherical mirror
and onto a spherical screen. The head-fixed animal moves in the virtual world
by walking on a polystyrene ball. This ball is suspended on a thin cushion of
air and its motion is detected by optical sensors. Our objective for the
visual stimuli in this system is to cover a 270\textdegree{} horizontal field of
view, from 135\textdegree{} left of center to 135\textdegree{} right of center,
and a 90\textdegree{} vertical field of view, from 20\textdegree{} below
horizontal to 70\textdegree{} above horizontal.


\subsection*{Viewing Direction Calculation}

A three dimensional unit vector is used to represent the viewing direction from
the animal to a point on the spherical screen.  Calculation of the viewing
direction corresponding to a point in the projector image is performed in three
steps as illustrated in Fig\ref{forward_mapping}. The first step is to
calculate the three dimensional Cartesian coordinates of point N where light
from a given point in the projector image strikes the mirror. This is
accomplished by considering the triangle consisting of the mirror's center,
point~M, the projector's focal point, point~P, and point~N. In this triangle
the vector from M to P is fully specified, the vector from M to N has an
unknown direction but a known length equal to the mirror radius, and the vector
from P to N has an unknown length but a known direction which is calculated for
the given point in the projector's image using parameters that characterize the
projector, see section ???. The law of cosines is used to find the coordinates
of point N as explained in section ???. The second step is to find the
coordinates of point E where the light reflected off the mirror strikes the
screen. This is the same problem as in step 1. In the triangle DEN the vector
from D to N is fully specified, the vector from D to E has an unknown direction
but a known length equal to the screen's radius, and the vector from N to E has
an unknown length but a known direction which is calculated from the direction
of the incoming light, the vector from P to N, and the direction of the vector
perpendicular to the surface of the mirror at the point of reflection, the
vector from M to N. See section ??? for the detailed calculation. Step~3 is
to use the triangle AEM to calculate the vector from A to E and normalize it to
get a unit vector that represents the viewing direction corresponding to the
given point in the projector's image.


This calculation requires quantitative values for several geometric propeties
of this virtual reality system. Specifically the location of the projector's
focal point, the aspect ratio of its image, an angle quantifying how much the
image spreads out horizontally as it gets farther from the projector and a
slope describing how far the image rises as it gets farther from the projector.
Also required are the radius of the spherical mirror, the radius of the
hemispherical screen, the location of the screen's center and the location of
the animal's head. It proved to be exceedingly difficult to obtain sufficiently
accurate values for all of these parameters through direct measurement.
Consider for instance that the focal point of the projector, the center of the
hemispherical screen and the center of the spherical mirror (the origin of our
chosen coordinate system) are not directly observable. We therefore developed
the calibration method described in the next section to find values of these
parameters which result in accurate calculated viewing directions.

\begin{eqnarray}
\label{eq:schemeP}
	\mathrm{P_Y} = \underbrace{H(Y_n) - H(Y_n|\mathbf{V}^{Y}_{n})}_{S_Y} +
                       \underbrace{H(Y_n|\mathbf{V}^{Y}_{n})-
                       H(Y_n|\mathbf{V}^{X,Y}_{n})}_{T_{X\rightarrow Y}},
\end{eqnarray}


\subsection*{Calibration Process}

After initially pursuing a camera based calibration method, we moved to a much
simpler method that utilizes a 3D printed calibration device. Camera based
methods require either fisheye lenses or numerous overlapping photographs (or
both) to achieve the wide field of view required to calibrate a murine virtual
reality system and correcting for camera distortion and the movement of the
camera's focal point between photos becomes complicated.  The calibration
device we use projects 37 spots of light in known directions, four rows of nine
spots plus one spot straight overhead. Each row of 9 spots has spots at yaw
angles of -120, -90, -60, -30, 0, 30, 60, 90, and 120 degrees where 0 degrees
is straight ahead, negative angles are to the left and positive angles are to
the right.  The four rows are at pitch angles of -15, 0, 30, and 60 degrees
where negative angles are downward and positive angles are upward. The
additional spot straight overhead has a pitch angle of 90 degrees. This
calibration device is placed at the animal's location inside the virtual
reality system and software developed specifically for this calibration method
is used to move green dots around in the projector's image until there's a
green dot in the center of each spot projected by the calibration device. This
software is then used to save a file containing the location of all the green
dots. This file constitutes a mapping between 37 points in the projector image
and 37 known viewing directions which can be used to find virtual reality
system geometry parameter values which produce accurate viewing directions
using the calculation procedure described in section\ref{}.  These parameter
values are found using an optimization method which minimizes the sum of the
square L2 norms of the vectors which constitute the differences between the
calculated and known viewing directions.

Insert table of viewing direction differences


\subsection{Image Modification}



\subsection*{Etiam eget sapien nibh.}

% For figure citations, please use "Fig" instead of "Figure".
Nulla mi mi, Fig~\ref{fig1} venenatis sed ipsum varius, volutpat euismod diam.

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
\caption{{\bf Bold the figure title.}
Figure caption text here, please use this space for the figure panel
descriptions instead of using subfigure commands. A: Lorem ipsum dolor sit
amet. B: Consectetur adipiscing elit.}
\label{fig1}
\end{figure}

% Results and Discussion can be combined.
\section*{Results}

% Place tables after the first paragraph in which they are cited.
\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Table caption Nulla mi mi, venenatis sed ipsum varius, volutpat euismod diam.}}
\begin{tabular}{|l+l|l|l|l|l|l|l|}
\hline
\multicolumn{4}{|l|}{\bf Heading1} & \multicolumn{4}{|l|}{\bf Heading2}\\ \thickhline
$cell1 row1$ & cell2 row 1 & cell3 row 1 & cell4 row 1 & cell5 row 1 & cell6 row 1 & cell7 row 1 & cell8 row 1\\ \hline
$cell1 row2$ & cell2 row 2 & cell3 row 2 & cell4 row 2 & cell5 row 2 & cell6 row 2 & cell7 row 2 & cell8 row 2\\ \hline
$cell1 row3$ & cell2 row 3 & cell3 row 3 & cell4 row 3 & cell5 row 3 & cell6 row 3 & cell7 row 3 & cell8 row 3\\ \hline
\end{tabular}
\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum
	mattis, massa tortor interdum felis, nec pellentesque metus tortor nec
	nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
\end{flushleft}
\label{table1}
\end{adjustwidth}
\end{table}


%PLOS does not support heading levels beyond the 3rd (no 4th level headings).
\subsection*{\lorem\ and \ipsum\ Nunc blandit a tortor.}
\subsubsection*{3rd Level Heading.} 

\begin{enumerate}
	\item{react}
	\item{diffuse free particles}
	\item{increment time by dt and go to 1}
\end{enumerate}


\subsection*{Sed ac quam id nisi malesuada congue.}


\begin{itemize}
	\item First bulleted item.
	\item Second bulleted item.
	\item Third bulleted item.
\end{itemize}

\section*{Discussion}

\section*{Conclusion}

information, see \nameref{S1_Appendix}.

\section*{Supporting Information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Fig.}
\label{S1_Fig}
{\bf Bold the title sentence.} Add descriptive text after the title of the item
(optional).

\paragraph*{S2 Fig.}
\label{S2_Fig}
{\bf Lorem Ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida.
Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod
ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 File.}
\label{S1_File}
{\bf Lorem Ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida.
Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod
ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Video.}
\label{S1_Video}
{\bf Lorem Ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida.
Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod
ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Appendix.}
\label{S1_Appendix}
{\bf Lorem Ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida.
Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod
ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Table.}
\label{S1_Table}
{\bf Lorem Ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida.
Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod
ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\section*{Acknowledgments}

This work was supported by NSF IOS-1450923, the Simons Foundation, and the
McNair Foundation.

\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here.
% 

\bibliography{bibliography}


\end{document}

